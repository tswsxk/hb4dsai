# 可解释机器学习

## 综述

### 定义

可解释人工智能（Explainable Artificial Intelligence）旨在于具备可为人类所理解的功能或运作机制，拥有**透明度**。

```
可解释性是硅基生物和碳基生物之间沟通的桥梁 	———— 鲁迅《这话我没说过》
```

可解释人工智能相较于其它黑盒模型而言，具有以下优势：

* 易调试
* 易改进：对机器学习模型的深入理解及结果的进一步提高
* 信任度高：在医疗、金融、军事、政治等关键领域，机器学习的结果影响甚大，而可解释机器学习则可以帮助决策者决定是否信任机器学习的结果。

#### 预备知识

##### 可解释性

###### interpretability vs explainability 

* interpretability 是指一个模型的被动特性，指的是一个给定的模型对人类观察者有意义的程度。这个特性也表示为透明性。
* explainability 可以被看作是模型的主动特征，表示模型为了阐明或详述其内部功能而采取的任何动作或过程。

###### 分层

* 不透明的系统: 其中从输入到输出的映射对用户来说是不可见的;
* 可解释系统: 用户可以对映射进行数学分析; 
* 可理解的系统: 在这个系统中，模型可以输出符号或规则以及它们的特定输出，以帮助理解映射背后的基本原理。

##### 术语

- Understandability (或等同地，intelligibility) 指的是一个模型的特征，使人理解其功能——模型如何工作——而不需要解释其内部结构或模型内部处理数据[18]的算法方法。


- Comprehensibility: 在ML模型中，可理解性是指学习算法以人类可理解的方式表示其已学知识的能力[19,20,21]。这种模型可理解性的概念源于Michalski[22]的假设，即“计算机归纳的结果应该是对给定实体的符号描述，在语义和结构上类似于人类专家可能产生的观察相同实体的结果。”这些描述的组成部分应作为单一的‘信息块’可理解，可直接用自然语言解释，并应以综合方式将定量和定性概念联系起来”。由于难以量化，可理解性通常与模型复杂度[17]的评估联系在一起。


- Interpretability 可解释性是指以可理解的语言向人类解释或提供意义的能力。


- Explainability 可解释性与作为人类和决策者之间的接口的解释概念相关，同时，这也是决策者的准确代理，也是人类可以理解的[17]。


- Transparency 透明度:如果一个模型本身是可以理解的，那么它就被认为是透明的。由于模型具有不同程度的可理解性，因此第3节中的透明模型分为三类: 可模拟模型、可分解模型和算法透明模型[5]。

### 必要性

近来，人工智能系统的复杂程度已经提高到几乎不需要人为干预来设计和部署它们。一般来说，考虑到对合乎道德的人工智能日益增长的需求，当来自这些系统的决策最终影响到人类的生活(例如，医学、法律或国防)时，人类就往往不愿采用不能直接解释、处理和信任的技术（这也是为什么在深度学习准确率这么高的情况下，仍然有一大部分人倾向于应用可解释性高的传统统计学模型的原因）。因此就有必要了解决策是如何由人工智能方法提供的。随着黑箱机器学习(ML)模型越来越多地被用于在关键环境中进行重要的预测，人工智能的各个利益相关者对透明度的要求也越来越高。危险在于做出和使用的决策不合理、不合法，或者不允许对其行为进行详细的解释。支持模型输出的解释是至关重要的，例如

- 在精准医疗中，为了支持诊断，专家需要从模型中获得远比简单的二进制预测多得多的信息，例如了解模型产生这样的判定是基于病人哪些因素的考虑。
- 其他例子包括交通、安全、金融等领域的自动驾驶汽车。

不难看出，人们其实希望可以知道模型究竟从数据中学到了哪些知识（以人类可以理解的方式表达的）从而产生了最终的决策。从中是不是可以帮助我们发现一些潜在的关联。如果一个模型完全不可解释，那么在很多领域的应用就会因为没办法给出更多可靠的信息而受到限制。

关于”可解释性在机器学习中是否必要“其实有很多讨论：

* 在NIPS 2017会场上，曾进行了一场非常激烈火爆的主题为“可解释性在机器学习中是否必要”的辩论，大家对可解释性的呼声还是非常高的。但人工智能三巨头之一的Yann LeCun却认为：**人类大脑是非常有限的，我们没有那么多脑容量去研究所有东西的可解释性**。有些东西是需要解释的，比如法律，但大多数情况下，它们并没有你想象中那么重要。比如世界上有那么多应用、网站，你每天用Facebook、Google的时候，你也没想着要寻求它们背后的可解释性。LeCun也举了一个例子：他多年前和一群经济学家也做了一个模型来预测房价。第一个用的简单的线性于猜测模型，经济学家也能解释清楚其中的原理；第二个用的是复杂的神经网络，但效果比第一个好上不少。结果，这群经济学家想要开公司做了。你说他们会选哪个？LeCun表示，任何时候在这两种里面选择都会选效果好的。就像很多年里虽然我们不知道药物里的成分但一直在用一样。
* 而 NIPS 2017年时间检验奖（Test-of-Time Award）获得者Rahimi在发表获奖感言的时候表示，现在的机器学习已经越来越像炼金术了。其实如果只是简单应用炼金术的结果倒也无妨，可是将类似炼金术的机器学习结果用于社交媒体甚至大选是不够严谨和周密的，这也让他感到不安。他的发言引起了很大的关注和争论。
* 他的质疑中提出的两个问题，我们将它们总结为透明度和信任度。
* 贝叶斯网络的创始人Pearl指出，**“几乎所有的深度学习突破性的本质上来说都只是些曲线拟合罢了”**，他认为今天人工智能领域的技术水平只不过是上一代机器已有功能的增强版。

从模型的性能和它的透明性之间的权衡来看，注重性能是正确的（正如LeCun说的那样）。但是值得注意的是，性能和透明性并不是完全对立的，**对一个系统理解的提高可以导致对其缺陷的修正**。同样，不可解释也意味着**危险**，事实上很多领域对深度学习模型应用的顾虑除了模型本身无法给出足够的信息之外，也有或多或少关于**安全性**的考虑。

- 对抗样本，对于一个CNN模型，在熊猫的图片中添加了一些噪声之后却以99.3%的概率被判定为长臂猿。![image](static/panda.jpg)



在开发ML模型时，将可解释性考虑为额外的设计驱动程序可以提高其可实现性:

- 可解释性有助于确保决策的公正性，即检测并纠正训练数据集中的偏差。
- 可解释性通过强调可能改变预测的潜在对抗性扰动，促进了稳健性的提供。
- 可解释性可以作为一种保证，即只有有意义的变量才能推断出输出，即，以确保模型推理中存在真实的因果关系。

这意味着，为了考虑实际，系统的解释应该

- 提供对模型机制和预测的理解，
- 或者提供模型识别规则的可视化，
- 或者提供可能扰乱模型的提示。

为了避免限制当前一代人工智能系统的有效性，可解释人工智能(XAI) 建议创建一套ML技术，

* 产生更多可解释的模型，同时保持高水平的学习性能 (如预测准确性)
* 使人类能够理解、适当信任和有效管理新一代人工智能伙伴

### 如何可解释

#### 划分

* 按过程划分
  0. 在建模之前的可解释性方法
  1. 建立本身具备可解释性的模型: 透明模型
  2. 在建模之后使用可解释性方法对模型作出解释 (后设技术，事后可解释，Post-hoc)
* 按模型相关性划分
  1. 模型相关
  2. 模型无关

#### 建立本身具备可解释性的模型

##### 透明模型

透明模型的三个层次

* 算法透明性
* 可分解性
* 可模拟性

目前已知的的可解释机器学习模型：线性回归、逻辑回归、决策树、GLMs、GAMs、KNNs

##### 基于规则的方法（Rule-based）

基于规则的方法比如我们提到的非常经典的决策树模型。这类模型中任何的一个决策都可以对应到一个逻辑规则表示。但当规则表示过多或者原始的特征本身就不是特别好解释的时候，基于规则的方法有时候也不太适用。

##### 基于单个特征的方法（Per-feature-based）

基于单个特征的方法主要是一些非常经典的线性模型，比如

* 线性回归
* 逻辑回归
* 广义线性回归
* 广义加性模型等

线性回归模型及其一些变种拥有非常solid的统计学基础，统计学可以说是最看重可解释性的一门学科了，上百年来无数数学家统计学家探讨了在各种不同情况下的模型的参数估计、参数修正、假设检验、边界条件等等问题，目的就是为了使得在各种不同情况下都能使模型具有有非常好的可解释性。

##### 基于实例的方法（Case-based）

基于实例的方法主要是通过一些代表性的样本来解释聚类/分类结果的方法。比如下图所展示的贝叶斯实例模型（Bayesian Case Model，BCM），我们将样本分成三个组团，可以分别找出每个组团中具有的代表性样例和重要的子空间。比如对于下面第一类聚类来说，绿脸是具有代表性的样本，而绿色、方块是具有代表性的特征子空间。基于实例的方法的一些局限在于可能挑出来的样本不具有代表性或者人们可能会有过度泛化的倾向。

##### 稀疏性方法（Sparsity）

基于稀疏性的方法主要是利用信息的稀疏性特质，将模型尽可能地简化表示。比如如下图的一种图稀疏性的LDA方法，根据层次性的单词信息形成了层次性的主题表达，这样一些小的主题就可以被更泛化的主题所概括，从而可以使我们更容易理解特定主题所代表的含义。

##### 单调性方法（Monotonicity）

基于单调性的方法：在很多机器学习问题中，有一些输入和输出之间存在正相关/负相关关系，如果在模型训练中我们可以找出这种单调性的关系就可以让模型具有更高的可解释性。比如医生对患特定疾病的概率的估计主要由一些跟该疾病相关联的高风险因素决定，找出单调性关系就可以帮助我们识别这些高风险因素。

#### 在建模之后使用可解释性方法对模型作出解释

- 用于事后解释的模型无关技术：可以无缝地应用于任何ML模型，而不考虑其内部处理或内部表示。目的是**从其预测过程中提取一些信息**。有时，**使用简化技术来生成模仿其前身的代理**（例如LIME），目的是为了获得易于处理和降低复杂性的东西。其他时候，意图集中在直接从模型中提取知识，或者简单地将它们可视化，以简化对其行为的解释。与模型无关的技术可能依赖于:
  - 模型简化
  - 特征相关性估计
  - 可视化
- 专为解释某些ML模型而定制或专门设计的事后解释能力。
  - 文本解释、可视化、局部解释、实例解释、简化解释和特征关联
  - 浅层ML模型：统称为所有不依赖于神经处理单元的分层结构的ML模型; Shallow ML覆盖了多种监督学习模型。在这些模型中，有一些严格可解释的(透明的)方法(如KNN和决策树，已经在第3节中讨论过)。考虑到它们在预测任务中的突出地位和显著性能，本节将集中讨论两种流行的浅ML模型(树集成和支持向量机，SVMs)，它们需要采用事后可解释性技术来解释它们的决策
  - 以及为深度学习模型设计的技术，这些技术相应地表示神经网络家族和相关变体，如卷积神经网络、递归神经网络和包含深度神经网络和透明模型的混合方案。
    - 事后局部解释
    - 特征相关技术正日益成为解释DNNs的主要方法。
    - 最常用的DL模型，即多层神经网络、卷积神经网络(CNN)和递归神经网络(RNN)的可解释性研究。

**1. 隐层分析方法**

**2. 模拟/代理模型**

**3. 敏感性分析方法**



#### 目前

### 阅读材料

[集大成者！可解释人工智能(XAI)研究最新进展万字综述论文: 概念体系机遇和挑战—构建负责任的人工智能](https://mp.weixin.qq.com/s/kJW-EM20C5JEQIKNtBYVtA)

[可解释人工智能XAI进展，看这份100页PPT为你讲解](https://mp.weixin.qq.com/s/QAMOg3woMTA8owTVZ0KUtg)

[AI可解释性文献列表](https://mp.weixin.qq.com/s/PPxfaiUzr7AHq4dZpO1Xgg)

[注意力机制不能提高模型可解释性？不，你这篇论文搞错了](https://mp.weixin.qq.com/s/lxluBY0N7c_whK6ttL1DCg)

[注意力机制可解释吗？这篇ACL 2019论文说……](https://mp.weixin.qq.com/s/79B6sNJ4hJI3ULuAQL4sRw)

[美国DARPA204页可解释人工智能文献综述论文《Explanation in Human-AI Systems》](https://mp.weixin.qq.com/s/o_IWb6veV1XK8LVvKT3Lkg)

[AAA2019 Tutorial：可解释AI—人工智能的圣杯（160页PPT从理论到动机，应用和局限性）](https://mp.weixin.qq.com/s/zZhRDzZGdoG59sde7dqfkg)

[【UC伯克利】可解释性机器学习：定义、方法和应用](https://mp.weixin.qq.com/s/HAy1mv9O3al9L7oM3Uzgog)

[打开人工智能黑箱:看最新16篇可解释深度学习文章，带您了解增强AI透明性](https://mp.weixin.qq.com/s/ZfcZ8anwa6fXYF0--t5Qrw)

[斯坦福完全可解释深度神经网络：你需要用决策树搞点事](https://mp.weixin.qq.com/s/ZIDiFrN3qQV4u-m9Wr3wiA)

[ICML 2018 | 训练可解释、可压缩、高准确率的LSTM](https://mp.weixin.qq.com/s/8BPZ_M8EGk3KxkSleYWSNw)

[深度学习的可解释性研究（一）—— 让模型具备说人话的能力 - 王小贱的文章 - 知乎](https://zhuanlan.zhihu.com/p/37223341)

### 参考文献



### 工具

[XAI](https://github.com/EthicalML/xai) [[中文文档]](https://mp.weixin.qq.com/s/XVl6voP5cwdC7DcvTMQvVQ)

## 白盒算法

### NLP

#### 阅读材料

[深度学习预训练模型可解释性概览](https://mp.weixin.qq.com/s/kJW-EM20C5JEQIKNtBYVtA)

#### 参考文献

## 可解释的语言模型



### 阅读材料

[ICML 最佳论文提名论文：理解词嵌入类比行为新方式](https://mp.weixin.qq.com/s/cExMngj2zjLeJyAHu72PGg)

[清华大学刘知远《知识指导的自然语言处理》，附55页PPT下载](https://mp.weixin.qq.com/s/eA24803jJp3DKs0H43a7Qw)

[把BERT拉下神坛！ACL论文只靠一个“Not”，就把AI阅读理解骤降到盲猜水平](https://mp.weixin.qq.com/s/r6bKZqg6z77zJN5RjqWnFA)

[自然语言理解（NLU）难在哪儿？](https://mp.weixin.qq.com/s/T31ZLZTOCyVQ5tjNvLasjg)

[BERT是否完美，语言模型又是否真正地「理解了语言」呢？](https://mp.weixin.qq.com/s/mA05VUJRfPx6dThcz52Vjg)

## 知识图谱用于增强模型可解释性

### 阅读材料

[【XAI2019】知识图谱中的可解释可验证表示学习，62页ppt](https://mp.weixin.qq.com/s/adhu1wKadGOtqIfNd9k6ZA)

[知识图谱在可解释人工智能中的作用，附81页ppt](https://mp.weixin.qq.com/s/na_9L-4yi-ocC1OYbrqluw)

[知识图谱如何用于可解释人工智能中？KGC2019这份教程为你讲解（附34页ppt）](https://mp.weixin.qq.com/s/3QjLFzMvH9qR05kmUyawQg)

## 可解释性推荐系统

### 阅读材料

[IJCAI 2019 | 为推荐系统生成高质量的文本解释：基于互注意力机制的多任务学习模型](https://mp.weixin.qq.com/s/iZgf3fapmaLBxicAYsCKlg)

[可解释推荐系统：知其然，知其所以然](https://mp.weixin.qq.com/s/7KXIbUfYC8VXD0reoWzi4w)

[可解释推荐系统：身怀绝技，一招击中用户心理](https://mp.weixin.qq.com/s/gQ9dV-IPWHTOLbI6u0D67g)

## 可解释性医疗系统

### 阅读材料

[【综述】医疗可解释人工智能综述论文](https://mp.weixin.qq.com/s/v9KzB-chVv43-YDfMLptKw)

## 可解释CV

### 阅读材料

[ICCV 2019教程《面向计算机视觉的可解释机器学习》，附280页PPT下载](https://mp.weixin.qq.com/s/m0-GW-D02Ryx7oXsO6vdpQ)

## 可解释法律系统

### 阅读材料

[【智能司法】可解释的Rationale增强罪名预测系统](https://mp.weixin.qq.com/s/Tn3GoYgTn42nUIF4Afru_g)